---
title: "ICL maximisation for discrete latent variable models"
author: "Etienne CÃ´me"
date: "23 mai 2019"
bibliography: bib_greed.bib
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE,out.extra=FALSE,message=FALSE}
library(greed)
library(Matrix)
library(ggpubr)
library(dplyr)
library(future)
plan(multiprocess)
```



## Introduction

Model based clustering is a principled approach for clustering [@Fraley2002], that has already proved to be very usefull in a variety of context thanks to it's capability of dealing with particular data structure. Such approaches encompasses to name a few: gaussian mixture models, mixture of regression and graph clustering models like stochastic block models. In their basic form such models assume that the observations $\{x_1,...x_n\}$ are drawn from a two step process, first a discrete latent variable is drawn using a multinomial distribution (with proportions $\pi$), then conditionaly on the drawn cluster an observation is sampled using the cluster own distribution. More formally, this can be written as:


$$
\begin{eqnarray}
\mathbf{z}_i|\pi&\sim& \mathcal{M}(1,\pi)\\
x_i|\mathbf{z}_{ik}=1,\theta_{k}&\sim& \mathcal{F}(\theta_{k})
\end{eqnarray}
$$

where the $\mathbf{z}_i$ is the so-called latent variable encoding cluster membership. From a computational perspective their are two mainb way to deal with such type of models. From a frequentist perspective, if the main focus is in estimating the data distribution to integrate out the latent variable and look for the $\theta$ that maximize the likelihood:

$$
\begin{eqnarray}
L(\theta_1,...,\theta_K)=\prod_{i=1}^N\sum_{k=1}^{K}\pi_k f(x_i;\theta_k)
\end{eqnarray}
$$

Such functional forms are not easy to optimize but the EM algorithm [@Dempster77] is a natural solution to this problem when the latent variable are independant given the observations. When this is not the case one may ressort to variational approximations [@Jordan1999] to solve the optimization problem. These type of approaches converge to a local maxima of the likelihood and require the  number of cluster to be fixed. When the number of groups $K$ is unknown, one classicaly try several values and ressorts to model selection criteria like BIC and AIC [] to find the best fitting model given the data at hands. These solution leads tipically to algorithms with complexity $\mathcal{O}(NK^2d)$ where $d$ is the number of features.

Always from a frequentist perspective when the main goal is to recovers the latent variable states one may keep them in the optimization problem and try to maximize the complete data likelihood with restect to both the latent variables sates and the distributions parameters. This leads to the Classification EM algorithm [@Celeux92] which generalize the K-means algorithm (which corresponds to the special case of a mixture of spherical gaussian with equal proportions). Such solution lead to algorithm with the same complexity as a classical EM even if they may converge more quickly.

From a Bayesian perpestive a natural way to deals with mixture model come from MCMC sampling which can be used to sample both latent variables values and parameters according to the posterior distribution. In such setting, the mixture model is complemented with prior distributions over parameters $(\pi,\theta_1,...,\theta_k)$, which are oftenly chosed to be conjugate with   :

$$
\begin{eqnarray}
\pi&\sim& \mathcal{D}(\mathbf{\alpha})\\
\mathbf{z}_i|\pi&\sim& \mathcal{M}(1,\pi)\\
\theta_{k}&\sim& P(\mathbf{\beta})\\
x_i|\mathbf{z}_{ik}=1,\theta_k&\sim& \mathcal{F}(\theta_{k})
\end{eqnarray}
$$

with $\mathcal{D}(\alpha)$ the dirichlet distribution of parameter $\alpha$ and $P(\beta)$ a prior distribution for the $\theta$'s. Using  gibbs sampling or variant one may obtain sample from the posterior distribution. However, sampling can be hard with difficulty to explore the different modes of the prosterior distribution. Explotation of the posterior samples can also be .  the .For a more complete discussion of this different and a complete state of the art see [@Celeux2018].


When 

Eventualy,    

$$
p(X,Z) = \int_{\theta_1,...,\theta_k}\int_{\pi}p(x|z,\theta)p(\theta)p(z|\pi)p(\pi)d\theta_1...d\theta_k d\pi
$$

- We propose and test several generic algorithm for solving
- We propose a general heuristic for getting extracting a regularization path with respect to $\alpha$. This strategie enable the extraction of a dendogramme and a partial ordering of the clsuter which is interesting on real datasets.







### Models 

#### Mixture models

#### Stochastick block models

#### Mixture of regression

### Algorithms

### Regularisation path and hierarchical clustering

## Experiments

### Simulated data


```{r}
N=1500
K=15
pi=rep(1/K,K)
lambda  = 0.1
lambda_o = 0.025
Ks=5
mu = bdiag(lapply(1:(K/Ks), function(k){matrix(lambda_o,Ks,Ks)+diag(rep(lambda,Ks))}))+0.001
sbm = rsbm(N,pi,mu)
```

```{r,cache=TRUE,echo=FALSE,message=FALSE,results="hide"}
fit = greed(sbm$x,model=new("sbm"),alg = new("hybrid",pop_size=20))
fit_o = greed::fit_greed(new("sbm"),list(X=sbm$x),sample(1:20,nrow(sbm$x),replace = TRUE))
fit_perm=fit
perm = sample(1:fit_perm@K)
fit_perm@obs_stats$counts=fit_perm@obs_stats$counts[perm]
fit_perm@obs_stats$x_counts=fit_perm@obs_stats$x_counts[perm,perm]
cl=spectral(sbm$x,15)
fit_sp=greed::fit_greed(new("sbm"),list(X=sbm$x),cl,type="none")
fit_sp_init=greed(sbm$x,model=new("sbm"),alg=new("seed"))
```

```{r,fig.height=8,fig.width=15,echo=FALSE}
p1=plot(fit_o)+ggtitle("greedy")
p2=plot(fit_perm)+ggtitle("hybrid")
p3=plot(fit)+ggtitle("hybrid + hierarchical ordering")
p4=plot(fit_o,type='nodelink')+ggtitle("")
p5=plot(fit_perm,type='nodelink')+ggtitle("")
p6=plot(fit,type='nodelink')+ggtitle("")
ggarrange(p1,p2,p3,p4,p5,p6,nrow = 2,ncol=3,common.legend = TRUE,legend="left")
```

```{r,fig.height=5,fig.width=10,echo=FALSE}
gen=ggplot(fit@train_hist)+geom_boxplot(aes(x=generation,y=icl,group=generation))+geom_point(aes(x=generation,y=icl,group=generation))+
  ggtitle("Evolution of ICL with respect generation")+
  theme_bw()
tree=plot(fit,type='tree')+ggtitle("Dendogramme derived from the best solution")
ggarrange(gen,tree)
```

```{r, cache=TRUE,message=FALSE,results="hide"}

Nbsim = 10
S=matrix(0,1500,Nbsim)
Fi=matrix(0,1500,Nbsim)
Fi_o=matrix(0,1500,Nbsim)
Fi_s=matrix(0,1500,Nbsim)
Fi_si=matrix(0,1500,Nbsim)
Fi_ms=matrix(0,1500,Nbsim)
Xlist = list()
for (s in 1:Nbsim){
  N=1500
  K=15
  pi=rep(1/K,K)
  lambda  = 0.1
  lambda_o = 0.025
  Ks=5
  mu = bdiag(lapply(1:(K/Ks), function(k){matrix(lambda_o,Ks,Ks)+diag(rep(lambda,Ks))}))+0.001
  sbm = rsbm(N,pi,mu)  
  S[,s]=sbm$cl
  Xlist[[s]]=sbm
  fit = greed(sbm$x,model=new("sbm"),alg = new("hybrid",pop_size=50))
  Fi[,s]=fit@cl
  fit_o = greed::fit_greed(new("sbm"),list(X=sbm$x),sample(1:20,nrow(sbm$x),replace = TRUE))
  Fi_o[,s] = fit_o@cl 
  cl=spectral(sbm$x,15)
  Fi_s[,s] = cl 
  fit_sp_init=greed(sbm$x,model=new("sbm"),alg=new("seed"))
  Fi_si[,s] = fit_sp_init@cl
  fit_ms = greed(sbm$x,model=new("sbm"),alg = new("multistarts",nb_start=50))
  Fi_ms[,s]=fit_ms@cl
}
```

```{r,echo=FALSE}
rFi = tibble(alg="Hybrid",nmi=sapply(1:Nbsim,function(s){NMI(Fi[,s],S[,s])}))
rFi_o = tibble(alg="Greedy",nmi=sapply(1:Nbsim,function(s){NMI(Fi_o[,s],S[,s])}))
rFi_s = tibble(alg="Spectral",nmi=sapply(1:Nbsim,function(s){NMI(Fi_s[,s],S[,s])}))
rFi_si = tibble(alg="Seed",nmi=sapply(1:Nbsim,function(s){NMI(Fi_si[,s],S[,s])}))
rFi_ms = tibble(alg="Multistart",nmi=sapply(1:Nbsim,function(s){NMI(Fi_ms[,s],S[,s])}))
resSim = rFi %>% bind_rows(rFi_o) %>% bind_rows(rFi_s)%>% bind_rows(rFi_si)%>% bind_rows(rFi_ms) %>% mutate(alg=factor(alg,levels=c("Greedy","Multistart","Spectral","Seed","Hybrid")))
```


```{r,fig.width=5,fig.height=5,echo=FALSE}
ggplot(resSim)+geom_boxplot(aes(x=alg,y=nmi,group=alg))+geom_point(aes(x=alg,y=nmi))+xlab("")+ylab("Normalized mutual information")+theme_bw()
```




### Real data


```{r, cache=TRUE,results="hide"}
data("Blogs")
fit_blogs = greed(Blogs$X,model=new("dcsbm"),alg=new("hybrid",prob_mutation=0))
data("Books")
fit_books = greed(Books$X,model=new("dcsbm"),alg=new("hybrid",prob_mutation=0))
data("Football")
fit_foot = greed(Football$X,model=new("dcsbm"),alg=new("hybrid",prob_mutation=0))
data("Jazz")
fit_jazz = greed(Jazz,model=new("dcsbm"),alg=new("hybrid",prob_mutation=0))
```


```{r, fig.height=16,fig.width=16, echo=FALSE}

p1=plot(fit_blogs)+ggtitle("Blogs")
p2=plot(fit_books)+ggtitle("Books")
p3=plot(fit_foot)+ggtitle("Football")
p4=plot(fit_jazz)+ggtitle("Jazz")

p5=plot(fit_blogs,type='nodelink')+ggtitle("")
p6=plot(fit_books,type='nodelink')+ggtitle("")
p7=plot(fit_foot,type='nodelink')+ggtitle("")
p8=plot(fit_jazz,type='nodelink')+ggtitle("")

p9=plot(fit_blogs,type='tree')+ggtitle("")
p10=plot(fit_books,type='tree')+ggtitle("")
p11=plot(fit_foot,type='tree')+ggtitle("")
p12=plot(fit_jazz,type='tree')+ggtitle("")


p13=plot(cut(fit_blogs,2))+ggtitle("")
p14=plot(cut(fit_books,3))+ggtitle("")
p15=plot(cut(fit_foot,10))+ggtitle("")
p16=plot(cut(fit_jazz,4))+ggtitle("")


ggarrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,nrow = 4,ncol=4,common.legend = TRUE)

```

## Conclusion

## References