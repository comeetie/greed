---
title: "Continuous data clustering with Gaussian mixtures"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{GMM}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Loads packages.
```{r setup}
library(greed)
library(mclust)
library(ggplot2)
set.seed(2134)
```

```{r, include=FALSE, echo=FALSE}
# set ggplot2's theme options globally
th = theme_classic(base_size = 14, base_family = base_family) %+replace%
  ggplot2::theme(
      # L'ensemble de la figure
      plot.title = element_text(size = rel(1), face = "bold", margin = margin(0,0,5,0), hjust = 0),
      # Zone où se situe le graphique
      # panel.grid.minor = element_blank(),
      # panel.border = element_blank(),
      # Les axes
      axis.title = element_text(size = rel(0.85), face = "bold"),
      axis.text = element_text(size = rel(0.70), face = "bold"),
      axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
      # La légende
      legend.title = element_text(size = rel(0.85), face = "bold"),
      legend.text = element_text(size = rel(0.70), face = "bold"),
      legend.key = element_rect(fill = "transparent", colour = NA),
      legend.key.size = unit(1.5, "lines"),
      legend.background = element_rect(fill = "transparent", colour = NA)
      )
ggplot2::theme_set(th)
```


Gaussian Mixture Models (GMMs) count among the most widely used [DLVMs](https://arxiv.org/abs/2002.11577) for continuous data clustering. 

## Full covariance matrix

Without any constraints, the Bayesian formulation of GMMs leading to a tractable exact ICL expression uses a Normal and inverse-Wishart conjugate prior on the mean and covariances $\param = (\mathbf{\mu}_k, \mathbf{\Sigma}_k)_k$. This prior is defined with hyperparameters $\bBeta = (\mathbf{\mu}, \tau, n_0, \mathbf{\varepsilon})$ and the hierarchical model is written as follows:
```{=tex}
\begin{equation}
\label{eq:gmm}
\begin{aligned}
\pi&\sim \textrm{Dirichlet}_K(\alpha)\\
Z_i&\sim \mathcal{M}(1,\pi)\\
\mathbf{\Sigma}_k^{-1} & \sim \textrm{Wishart}(\mathbf{\varepsilon}^{-1},n_0)\\
\mathbf{\mu}_k&\sim \mathcal{N}(\mathbf{\mu},\frac{1}{\tau} \mathbf{\Sigma}_k)\\
X_{i}|Z_{ik}=1 &\sim \mathcal{N}(\mathbf{\mu}_k, \mathbf{\Sigma}_{k})\\
\end{aligned}
\end{equation}
```

Contrary to other models, these priors are informative and may therefore have a sensible impact on the obtained results. By default, the priors parameters are set as follow:

- $\alpha=1$
- $\mu=\bar{X}$
- $\tau=0.01$
- $n_0=d$ 
- $\mathbf{\varepsilon}=0.1\,\textrm{diag}(\mathbf{\hat{\Sigma}}_{\mathbf{X}})$

For more details, we refer to the class documentation ``?Gmm``. These values were chosen to accommodate a variety of situation. Lets look at the results with the diabetes dataset:

```{r diabetes-gmm, fig.show='hold',out.width="90%",fig.width=8,fig.height=5.5}
data(diabetes)
X=diabetes[,-1]
sol = greed(X,model=Gmm())
table(diabetes$cl,clustering(sol))
```


To get an overview of the clustering results, you may use the `gmmpairs` plot function:
```{r diabetes-gmm-FIG, fig.show='hold',out.width="90%",fig.width=8,fig.height=5.5}
gmmpairs(sol,X) 
```


The obtained results are those expected and correspond to the known groups. If you want to experiments with other values for the prior parameters, the most important is $\mathbf{\varepsilon}$, which control the covariance matrix Whishart prior. You may try smaller values for example. In this case, you may also decrease $\tau$ to keep a flat priors on clusters means. For the diabetes data such choices leads to an interesting solution where one cluster is then used to fit one outlier.

```{r diabetes-gmm-0.01, fig.show='hold',out.width="90%",fig.width=8,fig.height=5.5}
sol = greed(X,model=Gmm(epsilon=0.01*diag(diag(cov(X))), tau =0.001))
gmmpairs(sol,X)
table(diabetes$cl,clustering(sol))
``` 


## Diagonal covariance matrix

You may also used diagonal covariance matrix. The ``?`diaggmm-class` `` allow to use such GMM type model. If you try such model on the previous diabetes dataset, greed this time found a 4 components GMM.

```{r diabetes-diaggmm, fig.show='hold',out.width="90%",fig.width=8,fig.height=8}
soldiag = greed(X,model=DiagGmm())
gmmpairs(soldiag,X)
table(diabetes$cl,clustering(soldiag))
```

You may still look at coarser clustering and inspect the clustering dendrogram:
```{r diabetes-diaggmm-cut, fig.show='hold',out.width="60%",fig.width=8,fig.height=6}
plot(soldiag,type='tree')
solK3 = cut(soldiag,3)
table(diabetes$cl,clustering(solK3))
```


```{r diabetes-diaggmm-cut-pairs, fig.show='hold',out.width="90%",fig.width=8,fig.height=8}
gmmpairs(solK3,X)
```

The clusters corresponds to the known group and if we compare the ICL of the, between the full model:
```{r icl-comp}
ICL(sol)
ICL(soldiag)
```

The full model seems preferable on this dataset. If you want to look at the mixture component parameters you may access their Maximum a posteriori estimate with the generic `coef` function.
```{r diabetes-diaggmm-cut-params}
params = coef(solK3)
params$Sigmak[[2]]
```


## The fashion MNIST dataset

Such simpler diagonal model may be of interest in particular for high dimensional settings for two reasons. First, the number of parameters (even if they are integrated out in the clustering phase) is reduced and this can be interesting when $d$ is important, but also because the prior maybe defined such that it will be less informative. You may try with a subset of the fashion mnist data provided with the package which contains 784 dimensional vectors (28x28 flattened images). In such settings, you may also want to switch the optimization algorithm to `` ?`seed-class` ``, this algorithm is less efficient than the hybrid algorithm used by default by greed. But, since it relied on a seeded initialization it is also a little bit less costly. In this case, you may increase the initial value for $K$ since, this algorithm is not able to find an clustering with a number of cluster bigger than the value of $K$ provided by the user. Still, it may simplify the clustering and return an optimal clustering with less clusters.    

```{r fashion-diaggmm}
data("fashion")
dim(fashion$X)
sol=greed(fashion$X,model=DiagGmm(),alg=Seed(),K=60)
```

On this more complex dataset, we may look at the dendrogram which is more interesting with the complex structure of these data.

```{r fashion-tree, fig.show='hold',out.width="60%",fig.width=8,fig.height=6}
plot(sol,type='tree')
```

Finally if we look at the clusters centers, they look coherent and thanks to the hierarchical ordering performed by greed are also well organized.

```{r fashion-means, fig.show='hold',out.width="90%",fig.width=8,fig.height=8}

clust_centers = coef(sol)$muk
im_list=lapply(1:sol@K,function(k){
    data.frame(i=rep(28:1,each=28),j=rep(1:28,28),v=t(clust_centers[[k]]),k=k)
    })

ims = do.call(rbind,im_list)
ggplot(ims)+
  geom_tile(aes(y=i,x=j,fill=v))+
  scale_fill_gradientn(colors=c("#ffffff","#000000"),guide="none")+
  scale_x_continuous(breaks=c())+scale_y_continuous(breaks=c())+facet_wrap(~k)+
  coord_equal()+theme_minimal()+
  theme(axis.title.x = element_blank(),axis.title.y = element_blank())
```

